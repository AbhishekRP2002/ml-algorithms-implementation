#### TODO: 

- Implement Polynomial Regression from scratch
- Implement `GD Variants` in Python 

#### Gradient Descent

- a  minimization optimization algorithm (aka SGD - Stochastic Gradient Descent)
- how will the code change for `maximisation` problem? `Gradient Ascent` code ? -- > change the Params update rules , is there anything more to it ?
- how can I implement `Gradient Descent with Momentum` , `Gradient Descent with Adaptive Gradients` or adding an `Adaptive Learning Rate  in the base GD algorithm ?`
